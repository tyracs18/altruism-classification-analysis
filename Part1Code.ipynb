{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4504ed7",
   "metadata": {},
   "source": [
    "## CS6474 Assignment III"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351b4c73",
   "metadata": {},
   "source": [
    "### Tyra Silaphet | 4-9-2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae0515a",
   "metadata": {},
   "source": [
    "#### Part 1a: Model 1 - n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a1d9b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Training samples: 5103\n",
      "Test samples: 568\n",
      "Class distribution (train): 24.63% positive\n",
      "\n",
      "Training model...\n",
      "\n",
      "Evaluating model performance...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.67      0.74       428\n",
      "           1       0.35      0.54      0.42       140\n",
      "\n",
      "    accuracy                           0.64       568\n",
      "   macro avg       0.58      0.60      0.58       568\n",
      "weighted avg       0.70      0.64      0.66       568\n",
      "\n",
      "\n",
      "Performance Metrics:\n",
      "Accuracy     0.637\n",
      "Precision    0.347\n",
      "Recall       0.536\n",
      "F1 Score     0.421\n",
      "Specificity  0.671\n",
      "AUC          0.620\n"
     ]
    }
   ],
   "source": [
    "# Part 1a: Analysis of Altruistic Request Success on Social Media Using SVM with N-gram Features\n",
    "\n",
    "import json\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                            f1_score, roc_auc_score, confusion_matrix,\n",
    "                            classification_report)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "def data_preprocessing_a(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)  # Load entire JSON array\n",
    "    \n",
    "    # Extract relevant fields (texts and labels)\n",
    "    texts = [d['request_text'] for d in data]\n",
    "    labels = [1 if d['requester_received_pizza'] else 0 for d in data]\n",
    "    \n",
    "    # Shuffle data to avoid any ordering effects\n",
    "    texts, labels = shuffle(texts, labels, random_state=123)\n",
    "    return texts, labels\n",
    "\n",
    "def evaluation(model, X_test, y_test):\n",
    "    # Generate predictions and probabilities\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Calculate required metrics using libraries \n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1 Score\": f1_score(y_test, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_test, y_proba)\n",
    "    }\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    metrics[\"Specificity\"] = tn / (tn + fp)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def main():\n",
    "    # Call preprocessing function\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    filepath = r'C:\\Users\\silap\\Desktop\\spring24\\cs6474\\Assignment3\\data\\pizza_request_dataset.json'\n",
    "    texts, labels = data_preprocessing_a(filepath)\n",
    "    \n",
    "    # Split data into training (90%) and test (10%) sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts, labels, \n",
    "        test_size=0.1, \n",
    "        random_state=42,\n",
    "        stratify=labels  # Address dataset imbalance\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    print(f\"Class distribution (train): {sum(y_train)/len(y_train):.2%} positive\")\n",
    "    \n",
    "    # Text processing pipeline with SVM\n",
    "    model = Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer(\n",
    "            ngram_range=(1,2), # 1=Unigrams, 2=bigrams\n",
    "            max_features=1000, # Select the top 1000 features (500 uni + 500 bi)\n",
    "            sublinear_tf=True, # Dampens effects of very frequent terms\n",
    "            stop_words='english',  # Remove common words\n",
    "            min_df=5,  # Ignore terms appearing in fewer than 5 documents\n",
    "            max_df=0.7  # Ignore terms appearing in >70% of documents\n",
    "        )),\n",
    "        \n",
    "        # SVM classifier with:\n",
    "        # - linear kernel for interpretability\n",
    "        # - class_weight='balanced' to handle imbalanced data\n",
    "        # - probability=True to enable ROC AUC calculation\n",
    "        ('classifier', SVC(\n",
    "            kernel='linear',\n",
    "            class_weight='balanced',\n",
    "            probability=True,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # 4. Train model\n",
    "    print(\"\\nTraining model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 5. Evaluate model\n",
    "    print(\"\\nEvaluating model performance...\")\n",
    "    metrics = evaluation(model, X_test, y_test)\n",
    "    \n",
    "    # Print results in a clean table format\n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    print(\"{:<12} {:.3f}\".format('Accuracy', metrics['Accuracy']))\n",
    "    print(\"{:<12} {:.3f}\".format('Precision', metrics['Precision']))\n",
    "    print(\"{:<12} {:.3f}\".format('Recall', metrics['Recall']))\n",
    "    print(\"{:<12} {:.3f}\".format('F1 Score', metrics['F1 Score']))\n",
    "    print(\"{:<12} {:.3f}\".format('Specificity', metrics['Specificity']))\n",
    "    print(\"{:<12} {:.3f}\".format('AUC', metrics['AUC']))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23267723",
   "metadata": {},
   "source": [
    "#### Part 1b: Model 2 - Activity and Reputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ec1c925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Training samples: 5103\n",
      "Test samples: 568\n",
      "Class distribution (train): 24.63% positive\n",
      "\n",
      "Training model...\n",
      "\n",
      "Evaluating model performance...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87       428\n",
      "           1       0.60      0.56      0.58       140\n",
      "\n",
      "    accuracy                           0.80       568\n",
      "   macro avg       0.73      0.72      0.73       568\n",
      "weighted avg       0.80      0.80      0.80       568\n",
      "\n",
      "\n",
      "Performance Metrics:\n",
      "Accuracy     0.801\n",
      "Precision    0.603\n",
      "Recall       0.564\n",
      "F1 Score     0.583\n",
      "Specificity  0.879\n",
      "AUC          0.839\n"
     ]
    }
   ],
   "source": [
    "# Part 1b: Analysis Using Activity and Reputation Features\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                            f1_score, roc_auc_score, confusion_matrix,\n",
    "                            classification_report)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "def feature_extraction(data):\n",
    "    features = []\n",
    "    \n",
    "    for post in data:\n",
    "        # Activity features\n",
    "        activity = [\n",
    "            int(post['post_was_edited']),\n",
    "            float(post['requester_account_age_in_days_at_request']),\n",
    "            float(post['requester_account_age_in_days_at_retrieval']),\n",
    "            float(post['requester_days_since_first_post_on_raop_at_request']),\n",
    "            float(post['requester_days_since_first_post_on_raop_at_retrieval']),\n",
    "            int(post['requester_number_of_comments_at_request']),\n",
    "            int(post['requester_number_of_comments_at_retrieval']),\n",
    "            int(post['requester_number_of_comments_in_raop_at_request']),\n",
    "            int(post['requester_number_of_comments_in_raop_at_retrieval']),\n",
    "            int(post['requester_number_of_posts_at_request']),\n",
    "            int(post['requester_number_of_posts_at_retrieval']),\n",
    "            int(post['requester_number_of_posts_on_raop_at_request']),\n",
    "            int(post['requester_number_of_posts_on_raop_at_retrieval']),\n",
    "            int(post['requester_number_of_subreddits_at_request']),\n",
    "            len(post['requester_subreddits_at_request']) if post['requester_subreddits_at_request'] != 'N/A' else 0\n",
    "        ]\n",
    "        \n",
    "        # Reputation features\n",
    "        reputation = [\n",
    "            int(post['number_of_downvotes_of_request_at_retrieval']),\n",
    "            int(post['number_of_upvotes_of_request_at_retrieval']),\n",
    "            int(post['requester_upvotes_minus_downvotes_at_request']),\n",
    "            int(post['requester_upvotes_minus_downvotes_at_retrieval']),\n",
    "            int(post['requester_upvotes_plus_downvotes_at_request']),\n",
    "            int(post['requester_upvotes_plus_downvotes_at_retrieval'])\n",
    "        ]\n",
    "\n",
    "        # Combine all features \n",
    "        features.append(activity + reputation)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def data_preprocessing(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    X = feature_extraction(data)\n",
    "    y = np.array([1 if post['requester_received_pizza'] else 0 for post in data])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def evaluation(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1 Score\": f1_score(y_test, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_test, y_proba)\n",
    "    }\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    metrics[\"Specificity\"] = tn / (tn + fp)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def main():\n",
    "    # Call preprocessing function\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    filepath = r'C:\\Users\\silap\\Desktop\\spring24\\cs6474\\Assignment3\\data\\pizza_request_dataset.json'\n",
    "    X, y = data_preprocessing(filepath)\n",
    "    \n",
    "    # Split data (90% train, 10% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    print(f\"Class distribution (train): {sum(y_train)/len(y_train):.2%} positive\")\n",
    "    \n",
    "    # Create pipeline with standardization and SVM\n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),  \n",
    "        ('classifier', SVC(\n",
    "            kernel='linear',\n",
    "            class_weight='balanced',\n",
    "            probability=True,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating model performance...\")\n",
    "    metrics = evaluation(model, X_test, y_test)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    print(\"{:<12} {:.3f}\".format('Accuracy', metrics['Accuracy']))\n",
    "    print(\"{:<12} {:.3f}\".format('Precision', metrics['Precision']))\n",
    "    print(\"{:<12} {:.3f}\".format('Recall', metrics['Recall']))\n",
    "    print(\"{:<12} {:.3f}\".format('F1 Score', metrics['F1 Score']))\n",
    "    print(\"{:<12} {:.3f}\".format('Specificity', metrics['Specificity']))\n",
    "    print(\"{:<12} {:.3f}\".format('AUC', metrics['AUC']))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3530a2",
   "metadata": {},
   "source": [
    "#### Part 1c: Model 3 - Narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1dd8fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Training samples: 5103\n",
      "Test samples: 568\n",
      "Class distribution (train): 24.63% positive\n",
      "\n",
      "Training model...\n",
      "\n",
      "Evaluating model performance...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.70      0.72       428\n",
      "           1       0.22      0.26      0.24       140\n",
      "\n",
      "    accuracy                           0.59       568\n",
      "   macro avg       0.48      0.48      0.48       568\n",
      "weighted avg       0.61      0.59      0.60       568\n",
      "\n",
      "\n",
      "Performance Metrics:\n",
      "Accuracy     0.590\n",
      "Precision    0.218\n",
      "Recall       0.257\n",
      "F1 Score     0.236\n",
      "Specificity  0.699\n",
      "AUC          0.495\n"
     ]
    }
   ],
   "source": [
    "# Part 1c: Analysis Using Narrative Dimensions Features\n",
    "\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                           f1_score, roc_auc_score, confusion_matrix,\n",
    "                           classification_report)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def load_narrative_words(narrative_paths):\n",
    "    narratives = {}\n",
    "    for name, path in narrative_paths.items():\n",
    "        with open(path, 'r') as f:\n",
    "            words = [line.strip() for line in f if line.strip()]\n",
    "            narratives[name] = words\n",
    "    return narratives\n",
    "\n",
    "def narrative_feature_extraction(texts, narrative_words):\n",
    "    features = []\n",
    "    for text in texts:\n",
    "        words = re.findall(r'\\w+', text.lower())\n",
    "        total_words = max(len(words), 1)  # Avoid division by zero\n",
    "        \n",
    "        narrative_counts = []\n",
    "        for name, word_list in narrative_words.items():\n",
    "            matches = sum(1 for word in words if word in word_list)\n",
    "            narrative_counts.append(matches / total_words)\n",
    "        \n",
    "        features.append(narrative_counts)\n",
    "    return np.array(features)\n",
    "\n",
    "def data_preprocessing(path, narrative_paths):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    texts = [d['request_text'] for d in data]\n",
    "    labels = [1 if d['requester_received_pizza'] else 0 for d in data]\n",
    "    \n",
    "    # Load narrative words\n",
    "    narrative_words = load_narrative_words(narrative_paths)\n",
    "    \n",
    "    # Extract features\n",
    "    X = narrative_feature_extraction(texts, narrative_words)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def main():\n",
    "    # Define paths to narrative files\n",
    "    narrative_paths = {\n",
    "        'desire': r'C:\\Users\\silap\\Desktop\\spring24\\cs6474\\Assignment3\\resources\\narratives\\desire.txt',\n",
    "        'family': r'C:\\Users\\silap\\Desktop\\spring24\\cs6474\\Assignment3\\resources\\narratives\\family.txt',\n",
    "        'job': r'C:\\Users\\silap\\Desktop\\spring24\\cs6474\\Assignment3\\resources\\narratives\\job.txt',\n",
    "        'money': r'C:\\Users\\silap\\Desktop\\spring24\\cs6474\\Assignment3\\resources\\narratives\\money.txt',\n",
    "        'student': r'C:\\Users\\silap\\Desktop\\spring24\\cs6474\\Assignment3\\resources\\narratives\\student.txt'\n",
    "    }\n",
    "    \n",
    "    # Call data preprocessing function\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    json_path = r'C:\\Users\\silap\\Desktop\\spring24\\cs6474\\Assignment3\\data\\pizza_request_dataset.json'\n",
    "    X, y = data_preprocessing(json_path, narrative_paths)\n",
    "    \n",
    "    # Split data (90% train, 10% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    print(f\"Class distribution (train): {sum(y_train)/len(y_train):.2%} positive\")\n",
    "    \n",
    "    # Create pipeline with standardization and SVM\n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('svm', SVC(\n",
    "            kernel='linear',\n",
    "            class_weight='balanced',\n",
    "            probability=True,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating model performance...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1 Score\": f1_score(y_test, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_test, y_proba)\n",
    "    }\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    metrics[\"Specificity\"] = tn / (tn + fp)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    print(\"{:<12} {:.3f}\".format('Accuracy', metrics['Accuracy']))\n",
    "    print(\"{:<12} {:.3f}\".format('Precision', metrics['Precision']))\n",
    "    print(\"{:<12} {:.3f}\".format('Recall', metrics['Recall']))\n",
    "    print(\"{:<12} {:.3f}\".format('F1 Score', metrics['F1 Score']))\n",
    "    print(\"{:<12} {:.3f}\".format('Specificity', metrics['Specificity']))\n",
    "    print(\"{:<12} {:.3f}\".format('AUC', metrics['AUC']))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a86053",
   "metadata": {},
   "source": [
    "#### Part 1d: Model 4 - Moral Foundations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d856a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Training samples: 5103\n",
      "Test samples: 568\n",
      "Class distribution (train): 24.63% positive\n",
      "\n",
      "Training model...\n",
      "\n",
      "Evaluating model performance...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.96      0.84       428\n",
      "           1       0.24      0.04      0.07       140\n",
      "\n",
      "    accuracy                           0.73       568\n",
      "   macro avg       0.50      0.50      0.46       568\n",
      "weighted avg       0.63      0.73      0.65       568\n",
      "\n",
      "\n",
      "Performance Metrics:\n",
      "Accuracy     0.731\n",
      "Precision    0.240\n",
      "Recall       0.043\n",
      "F1 Score     0.073\n",
      "Specificity  0.956\n",
      "AUC          0.464\n"
     ]
    }
   ],
   "source": [
    "# Part 1d: Analysis Using Moral Foundations Features\n",
    "\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                           f1_score, roc_auc_score, confusion_matrix,\n",
    "                           classification_report)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def load_moral_dictionary(dict_path):\n",
    "    foundation_map = {\n",
    "        '01': 'care',\n",
    "        '02': 'harm',\n",
    "        '03': 'fairness',\n",
    "        '04': 'unfairness',\n",
    "        '05': 'loyalty',\n",
    "        '06': 'betrayal',\n",
    "        '07': 'authority',\n",
    "        '08': 'subversion',\n",
    "        '09': 'purity',  # This was previously called 'sanctity' in your code\n",
    "        '10': 'degradation',\n",
    "        '11': 'morality_general'\n",
    "    }\n",
    "    \n",
    "    foundations = {v: [] for v in foundation_map.values()}\n",
    "    \n",
    "    with open(dict_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('%'):\n",
    "                continue\n",
    "                \n",
    "            parts = line.split()\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "                \n",
    "            word = parts[0].replace('*', '')\n",
    "            codes = parts[1:]\n",
    "            \n",
    "            for code in codes:\n",
    "                if code in foundation_map:\n",
    "                    foundation = foundation_map[code]\n",
    "                    if word not in foundations[foundation]:  # Avoid duplicates\n",
    "                        foundations[foundation].append(word)\n",
    "    \n",
    "    # Verify we loaded words for all required foundations\n",
    "    required_foundations = ['care', 'fairness', 'loyalty', 'authority', 'purity']\n",
    "    for foundation in required_foundations:\n",
    "        if not foundations.get(foundation):\n",
    "            print(f\"Warning: No words loaded for foundation {foundation}\")\n",
    "    \n",
    "    return {k: v for k, v in foundations.items() if k in required_foundations}\n",
    "\n",
    "def moral_feature_extraction(texts, foundations):\n",
    "    features = []\n",
    "    \n",
    "    for text in texts:\n",
    "        words = re.findall(r'\\w+', text.lower())\n",
    "        total_words = max(len(words), 1)  # Avoid division by zero\n",
    "        \n",
    "        foundation_counts = []\n",
    "        # Use consistent foundation order\n",
    "        for foundation in ['care', 'fairness', 'loyalty', 'authority', 'purity']:\n",
    "            matches = sum(1 for word in words if word in foundations[foundation])\n",
    "            foundation_counts.append(matches / total_words)\n",
    "        \n",
    "        features.append(foundation_counts)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def data_preprocessing(json_path, dict_path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    texts = [d['request_text'] for d in data]\n",
    "    labels = [1 if d['requester_received_pizza'] else 0 for d in data]\n",
    "    \n",
    "    # Load moral foundations dictionary\n",
    "    foundations = load_moral_dictionary(dict_path)\n",
    "    \n",
    "    # Extract features\n",
    "    X = moral_feature_extraction(texts, foundations)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def main():\n",
    "    # Define paths\n",
    "    json_path = r'C:\\Users\\silap\\Desktop\\spring24\\cs6474\\Assignment3\\data\\pizza_request_dataset.json'\n",
    "    dict_path = r'C:\\Users\\silap\\Desktop\\spring24\\cs6474\\Assignment3\\resources\\MoralFoundations.dic'\n",
    "    \n",
    "    # Call data preprocessing function\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    X, y = data_preprocessing(json_path, dict_path)\n",
    "    \n",
    "    # Split data (90% train, 10% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    print(f\"Class distribution (train): {sum(y_train)/len(y_train):.2%} positive\")\n",
    "    \n",
    "    # Create pipeline with standardization and SVM\n",
    "    model = Pipeline([\n",
    "        ('scaler', StandardScaler()),  \n",
    "        ('svm', SVC(\n",
    "            kernel='linear',\n",
    "            class_weight='balanced',\n",
    "            probability=True,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating model performance...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1 Score\": f1_score(y_test, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_test, y_proba)\n",
    "    }\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    metrics[\"Specificity\"] = tn / (tn + fp)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    print(\"{:<12} {:.3f}\".format('Accuracy', metrics['Accuracy']))\n",
    "    print(\"{:<12} {:.3f}\".format('Precision', metrics['Precision']))\n",
    "    print(\"{:<12} {:.3f}\".format('Recall', metrics['Recall']))\n",
    "    print(\"{:<12} {:.3f}\".format('F1 Score', metrics['F1 Score']))\n",
    "    print(\"{:<12} {:.3f}\".format('Specificity', metrics['Specificity']))\n",
    "    print(\"{:<12} {:.3f}\".format('AUC', metrics['AUC']))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
